!This is where all of the PGI CUDA FORTRAN code will go, and these routines will be called from prim_advection_mod.
!This is compiled regardless, but PGI-specific calls are always wrapped in the _ACCEL ifdefs that are automagically
!activated when -Mcuda is specified during compilation with a PGI compiler. Thus, it will be ignored unless explicitly
!activated by the user
!
!As a general rule, all of the routines in here will be called within a threaded context (assuming ELEMENT_OPENMP is not
!deifned), and therefore, we enforce BARRIERS, MASTERS, and SINGLES from within these routines rather than outside them.
!This is to minimize the visible code impacts on the existing CPU code.

! Please pay attention to this all caps passive aggresive banner.
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!                     !!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!  STATUS INCOMPLETE  !!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!  DO NOT USE YET     !!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!  UNTIL THIS BANNER  !!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!  IS REMOVED         !!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!                     !!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

module cuda_mod
#ifdef _ACCEL
#define PAD 1

!Put everything CUDA-specific in here so it doesn't get compiled without -Mcuda enabled on a PGI compiler
  use cudafor
  use kinds          , only: real_kind
  use dimensions_mod , only: np,nlevp,nlev,qsize,qsize_d,max_corner_elem,max_neigh_edges,nelemd
  use element_mod    , only: timelevels
  use edge_mod       , only: EdgeBuffer_t
  implicit none
  private

  !First listed are all externally accescible routines
  public :: cuda_mod_init
  public :: euler_step_cuda 

  !This is from prim_advection_mod.F90
  type(EdgeBuffer_t) :: edgeAdv, edgeAdvQ3, edgeAdvQ2, edgeAdvDSS
  real(kind=real_kind), allocatable :: qmin(:,:,:), qmax(:,:,:)
  integer,parameter :: DSSeta = 1
  integer,parameter :: DSSomega = 2
  integer,parameter :: DSSdiv_vdp_ave = 3
  integer,parameter :: DSSno_var = -1

  !Device arrays
  real (kind=real_kind),device,allocatable,dimension(:,:,:,:,:,:) :: qdp_d
  real (kind=real_kind),device,allocatable,dimension(:,:,:,:,:)   :: qtens_d
  real (kind=real_kind),device,allocatable,dimension(:,:,:)       :: spheremp_d
  real (kind=real_kind),device,allocatable,dimension(:,:,:)       :: rspheremp_d
  real (kind=real_kind),device,allocatable,dimension(:,:,:,:,:)   :: dinv_d
  real (kind=real_kind),device,allocatable,dimension(:,:,:)       :: variable_hyperviscosity_d
  real (kind=real_kind),device,allocatable,dimension(:,:,:)       :: metdet_d
  real (kind=real_kind),device,allocatable,dimension(:,:,:)       :: rmetdet_d
  real (kind=real_kind),device,allocatable,dimension(:,:)         :: edgebuf_d
  logical              ,device,allocatable,dimension(:,:)         :: reverse_d
  integer              ,device,allocatable,dimension(:,:)         :: putmapP_d
  integer              ,device,allocatable,dimension(:,:)         :: getmapP_d
  real (kind=real_kind),device,allocatable,dimension(:,:,:,:,:)   :: vstar_d
  real (kind=real_kind),device,allocatable,dimension(:,:,:,:)     :: dp_d
  integer              ,device,allocatable,dimension(:)           :: send_nelem_d
  integer              ,device,allocatable,dimension(:)           :: recv_nelem_d
  integer              ,device,allocatable,dimension(:,:)         :: send_indices_d
  integer              ,device,allocatable,dimension(:,:)         :: recv_indices_d
  integer              ,device,allocatable,dimension(:)           :: recv_internal_indices_d
  integer              ,device,allocatable,dimension(:)           :: recv_external_indices_d
  real (kind=real_kind),device,allocatable,dimension(:,:,:)       :: recvbuf_d

  !PINNED Host arrays
  real(kind=real_kind),pinned,allocatable,dimension(:,:,:,:,:) :: Vstar_h
  real(kind=real_kind),pinned,allocatable,dimension(:,:,:,:) :: dp_h
  real(kind=real_kind),pinned,allocatable,dimension(:,:,:,:) :: dp_star_h
  real(kind=real_kind),pinned,allocatable,dimension(:,:,:,:) :: dp_np1_h
  real(kind=real_kind),pinned,allocatable,dimension(:,:,:) :: sendbuf_h
  real(kind=real_kind),pinned,allocatable,dimension(:,:,:) :: recvbuf_h

  !Normal Host arrays
  integer,allocatable,dimension(:)   :: send_nelem
  integer,allocatable,dimension(:)   :: recv_nelem
  integer,allocatable,dimension(:,:) :: send_indices
  integer,allocatable,dimension(:,:) :: recv_indices
  integer,allocatable,dimension(:)   :: recv_internal_indices
  integer,allocatable,dimension(:)   :: recv_external_indices
  integer :: recv_external_nelem
  integer :: recv_internal_nelem
  logical :: old_peu
  logical,allocatable,dimension(:)   :: d2h_done
  logical,allocatable,dimension(:)   :: msg_sent
  logical,allocatable,dimension(:)   :: msg_rcvd
  logical,allocatable,dimension(:)   :: h2d_done
  integer, parameter :: south_px = 1
  integer, parameter :: east_px  = 3
  integer, parameter :: north_px = 4
  integer, parameter :: west_px  = 2
  integer, parameter :: cuda_streams = 16
  integer            :: streams(0:cuda_streams)
  integer            :: streams2(0:cuda_streams)
  integer            :: nbuf
  integer            :: nmsg_rcvd
  integer            :: nmsg_sent

  type(cudaEvent) :: timer1, timer2


contains



  !The point of this is to initialize any data required in other routines of this module as well
  !as to run one initial CUDA kernel just to get those overheads out of the way so that subsequent
  !timing routines are accurage.
  subroutine cuda_mod_init(elem)
    use edge_mod      , only: initEdgeBuffer
    use schedule_mod  , only: schedule_t, cycle_t, schedule
    use edge_mod      , only: Edgebuffer_t
    use element_mod   , only: element_t
    implicit none
    type(element_t), intent(in) :: elem(:)

    type (Cycle_t),pointer    :: pCycle
    type (Schedule_t),pointer :: pSchedule
    integer                   :: ie , ierr , icycle , iPtr , rank , nSendCycles , nRecvCycles , nlyr , mx_send_len , mx_recv_len , n
    real(kind=real_kind)      :: dinv_t(np , np , 2 , 2)
    type (dim3)               :: griddim , blockdim
    logical,allocatable,dimension(:,:) :: send_elem_mask
    logical,allocatable,dimension(:,:) :: recv_elem_mask
    logical,allocatable,dimension(:)   :: elem_computed
    integer :: total_work

#if (defined ELEMENT_OPENMP)
    write(*,*) 'ERROR: Do not use ELEMENT_OPENMP and CUDA FORTRAN'
    stop
#endif
!$OMP BARRIER
!$OMP MASTER
    write(*,*) "cuda_mod_init"

    write(*,*) "allocate arrays on device & host"
#ifdef _PREDICT
    pSchedule => Schedule(iam)
#else
    pSchedule => Schedule(1)
#endif
    nlyr = edgeAdv%nlyr
    nSendCycles = pSchedule%nSendCycles
    nRecvCycles = pSchedule%nRecvCycles
    mx_send_len = 0
    mx_recv_len = 0
    do icycle=1,nSendCycles
      if (pSchedule%SendCycle(icycle)%lengthP > mx_send_len) mx_send_len = pSchedule%SendCycle(icycle)%lengthP
    enddo
    do icycle=1,nRecvCycles
      if (pSchedule%RecvCycle(icycle)%lengthP > mx_recv_len) mx_recv_len = pSchedule%RecvCycle(icycle)%lengthP 
    enddo
    nbuf=4*(np+max_corner_elem)*nelemd  !inlined from edge_mod.F90, initEdgeBuffer()

    !Allocate the host and device arrays
    allocate( qmin                     (nlev,qsize_d                 ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( qmax                     (nlev,qsize_d                 ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( qdp_d                    (np,np,nlev,qsize_d,timelevels,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( qtens_d                  (np,np,nlev,qsize_d           ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( spheremp_d               (np,np                        ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( rspheremp_d              (np,np                        ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( dinv_d                   (np,np,2,2                    ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( variable_hyperviscosity_d(np,np                        ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( metdet_d                 (np,np                        ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( rmetdet_d                (np,np                        ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( vstar_d                  (np,np,nlev,2                 ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( dp_d                     (np,np,nlev                   ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( reverse_d                (max_neigh_edges              ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( putmapP_d                (max_neigh_edges              ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( getmapP_d                (max_neigh_edges              ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( recvbuf_d                (nlev*qsize_d,mx_recv_len,nRecvCycles) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( edgebuf_d                (nlev*qsize_d,nbuf                   ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( send_nelem_d             (       nSendCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( recv_nelem_d             (       nRecvCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( send_indices_d           (nelemd,nSendCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( recv_indices_d           (nelemd,nRecvCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( recv_internal_indices_d  (nelemd                              ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( recv_external_indices_d  (nelemd                              ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__

    allocate( Vstar_h                  (np,np,nlev,2                 ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( dp_h                     (np,np,nlev                   ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( dp_star_h                (np,np,nlev                   ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( dp_np1_h                 (np,np,nlev                   ,nelemd) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( sendbuf_h                (nlev*qsize_d,mx_send_len,nSendCycles) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( recvbuf_h                (nlev*qsize_d,mx_recv_len,nRecvCycles) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( send_elem_mask           (nelemd,nSendCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( recv_elem_mask           (nelemd,nRecvCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( send_nelem               (       nSendCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( recv_nelem               (       nRecvCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( send_indices             (nelemd,nSendCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( recv_indices             (nelemd,nRecvCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( recv_internal_indices    (nelemd                              ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( recv_external_indices    (nelemd                              ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( send_elem_mask           (nelemd,nSendCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( recv_elem_mask           (nelemd,nRecvCycles                  ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( elem_computed            (nelemd                              ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( d2h_done                 (nSendCycles                         ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( msg_sent                 (nSendCycles                         ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( msg_rcvd                 (nRecvCycles                         ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__
    allocate( h2d_done                 (nRecvCycles                         ) , stat = ierr ) ; if ( ierr .ne. 0 ) stop __LINE__

    write(*,*) "send data from host to device"
    !Copy over data to the device
    do ie = 1,nelemd
      dinv_t(:,:,1,1) = elem(ie)%dinv(1,1,:,:)
      dinv_t(:,:,1,2) = elem(ie)%dinv(1,2,:,:)
      dinv_t(:,:,2,1) = elem(ie)%dinv(2,1,:,:)
      dinv_t(:,:,2,2) = elem(ie)%dinv(2,2,:,:)
      ierr = cudaMemcpy( qdp_d                    (1,1,1,1,1,ie) , elem(ie)%state%Qdp               , size(elem(ie)%state%Qdp              ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
      ierr = cudaMemcpy( spheremp_d               (1,1      ,ie) , elem(ie)%spheremp                , size(elem(ie)%spheremp               ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
      ierr = cudaMemcpy( rspheremp_d              (1,1      ,ie) , elem(ie)%rspheremp               , size(elem(ie)%rspheremp              ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
      ierr = cudaMemcpy( dinv_d                   (1,1,1,1  ,ie) , dinv_t                           , size(dinv_t                          ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
      ierr = cudaMemcpy( metdet_d                 (1,1      ,ie) , elem(ie)%metdet                  , size(elem(ie)%metdet                 ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
      ierr = cudaMemcpy( rmetdet_d                (1,1      ,ie) , elem(ie)%rmetdet                 , size(elem(ie)%rmetdet                ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
      ierr = cudaMemcpy( putmapP_d                (1        ,ie) , elem(ie)%desc%putmapP            , size(elem(ie)%desc%putmapP           ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
      ierr = cudaMemcpy( getmapP_d                (1        ,ie) , elem(ie)%desc%getmapP            , size(elem(ie)%desc%getmapP           ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
      ierr = cudaMemcpy( reverse_d                (1        ,ie) , elem(ie)%desc%reverse            , size(elem(ie)%desc%reverse           ) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
      ierr = cudaMemcpy( variable_hyperviscosity_d(1,1      ,ie) , elem(ie)%variable_hyperviscosity , size(elem(ie)%variable_hyperviscosity) , cudaMemcpyHostToDevice ) ; if ( ierr .ne. 0 ) stop __LINE__
    enddo

    write(*,*) "edgebuffers"
    !These have to be in a threaded region or they complain and die
!$OMP END MASTER
    call initEdgeBuffer(edgeAdv   ,qsize_d*nlev  )
    call initEdgeBuffer(edgeAdvDSS,      nlev  )
    call initEdgeBuffer(edgeAdvQ2 ,qsize_d*nlev*2)
    call initEdgeBuffer(edgeAdvQ3 ,qsize_d*nlev*3)
!$OMP MASTER

    write(*,*) "initial kernel"
    !This needs to run because we need accurate timing during out cuda profiler runs. Initial kernel runs incur overheads, so we do this here
    blockdim = dim3(1,1,1)
    griddim  = dim3(1,1,1)
    call warmup <<< griddim , blockdim >>> ( ie )
    ierr = cudaThreadSynchronize()

    do n = 0 , cuda_streams
      ierr = cudaStreamCreate(streams(n))
      ierr = cudaStreamCreate(streams2(n))
    enddo
    ierr = cudaDeviceSetCacheConfig(cudaFuncCachePreferShared)
    ierr = cudaEventCreate(timer1)
    ierr = cudaEventCreate(timer2)

    write(*,*) "Dividing elements among cycles in which they participate"
    !For efficient MPI, PCI-e, packing, and unpacking, we need to separate out the cycles by dependence. Once on cycle has packed, then stage the PCI-e D2H, MPI, PCI-e H2D, & internal unpack
    !We begin by testing what elements contribute to packing in what cycle's MPI data.
    do ie = 1,nelemd
      send_elem_mask(ie,:) = .false.
      do icycle = 1 , nSendCycles
        do n = 1 , max_neigh_edges
          if ( elem(ie)%desc%putmapP(n) >= pSchedule%SendCycle(icycle)%ptrP .and. &
               elem(ie)%desc%putmapP(n) <= pSchedule%SendCycle(icycle)%ptrP + pSchedule%SendCycle(icycle)%lengthP-1 ) then
            send_elem_mask(ie,icycle) = .true.
          endif
        enddo
      enddo
      recv_elem_mask(ie,:) = .false.
      do icycle = 1 , nRecvCycles
        do n = 1 , max_neigh_edges
          if ( elem(ie)%desc%getmapP(n) >= pSchedule%RecvCycle(icycle)%ptrP .and. &
               elem(ie)%desc%getmapP(n) <= pSchedule%RecvCycle(icycle)%ptrP + pSchedule%RecvCycle(icycle)%lengthP-1 ) then
            recv_elem_mask(ie,icycle) = .true.
          endif
        enddo
      enddo
    enddo
    edgebuf_d = 0.

    elem_computed = .false.   !elem_computed tells us whether an element has been touched by a cycle
    !This pass accumulates for each cycle incides participating in the MPI_Isend
    do icycle = 1 , nSendCycles
      send_nelem(icycle) = 0
      do ie = 1 , nelemd
        if ( send_elem_mask(ie,icycle) ) then
          send_nelem(icycle) = send_nelem(icycle) + 1
          send_indices(send_nelem(icycle),icycle) = ie
          elem_computed(ie) = .true.
        endif
      enddo
    enddo
    total_work = sum(send_nelem)
    do ie = 1 , nelemd
      if (.not. elem_computed(ie)) total_work = total_work + 1
    enddo
    !This pass adds to each cycle the internal elements not participating in MPI_Isend, so as to even distribute them across cycles.
    do icycle = 1 , nSendCycles
      do ie = 1 , nelemd
        if ( .not. elem_computed(ie) .and. send_nelem(icycle) < int(ceiling(total_work/dble(nSendCycles))) ) then
          send_nelem(icycle) = send_nelem(icycle) + 1
          send_indices(send_nelem(icycle),icycle) = ie
          elem_computed(ie) = .true.
        endif
      enddo
    enddo

    elem_computed = .false.
    !This pass accumulates for each cycle incides participating in the MPI_Irecv
    do icycle = 1 , nRecvCycles
      recv_nelem(icycle) = 0
      do ie = 1 , nelemd
        if ( recv_elem_mask(ie,icycle) .and. ( .not. elem_computed(ie) ) ) then
          recv_nelem(icycle) = recv_nelem(icycle) + 1
          recv_indices(recv_nelem(icycle),icycle) = ie
          elem_computed(ie) = .true.
        endif
      enddo
    enddo
    !This pass accumulates all elements from all cycles participating in MPI_Irecv into the recv_external_indices array
    recv_external_nelem = 0
    do icycle = 1 , nRecvCycles
      do ie = 1 , recv_nelem(icycle)
        recv_external_nelem = recv_external_nelem + 1
        recv_external_indices(recv_external_nelem) = recv_indices(ie,icycle)
      enddo
    enddo
    !This pass goes through all elements, and distributes evenly the elements not participating in MPI_Irecv 
    recv_internal_nelem = 0
    do ie = 1 , nelemd
      if ( .not. elem_computed(ie) ) then
        recv_internal_nelem = recv_internal_nelem + 1
        recv_internal_indices(recv_internal_nelem) = ie
      endif
    enddo
    !This pass adds to each cycle the internal elements not participating in MPI_Irecv, so as to even distribute them across cycles.
    do icycle = 1 , nRecvCycles
      do ie = 1 , nelemd
        if ( .not. elem_computed(ie) .and. recv_nelem(icycle) < int(ceiling(nelemd/dble(nRecvCycles))) ) then
          recv_nelem(icycle) = recv_nelem(icycle) + 1
          recv_indices(recv_nelem(icycle),icycle) = ie
          elem_computed(ie) = .true.
        endif
      enddo
    enddo

    old_peu = .false.
    do icycle = 1 , nSendCycles
      if (send_nelem(icycle) == 0) then
        write(*,*) 'WARNING: ZERO ELEMENT CYCLES EXIST. A BETTER DECOMPOSITION WILL RUN FASTER IN THE PACK-EXCHANGE-UNPACK.'
        old_peu = .true.
      endif
    enddo

    write(*,*) "Sending element & cycle informationt to device"
    ierr = cudaMemcpy(send_nelem_d           ,send_nelem           ,size(send_nelem           ),cudaMemcpyHostToDevice); if(ierr.ne.0) stop __LINE__
    ierr = cudaMemcpy(recv_nelem_d           ,recv_nelem           ,size(recv_nelem           ),cudaMemcpyHostToDevice); if(ierr.ne.0) stop __LINE__
    ierr = cudaMemcpy(send_indices_d         ,send_indices         ,size(send_indices         ),cudaMemcpyHostToDevice); if(ierr.ne.0) stop __LINE__
    ierr = cudaMemcpy(recv_indices_d         ,recv_indices         ,size(recv_indices         ),cudaMemcpyHostToDevice); if(ierr.ne.0) stop __LINE__
    ierr = cudaMemcpy(recv_internal_indices_d,recv_internal_indices,size(recv_internal_indices),cudaMemcpyHostToDevice); if(ierr.ne.0) stop __LINE__
    ierr = cudaMemcpy(recv_external_indices_d,recv_external_indices,size(recv_external_indices),cudaMemcpyHostToDevice); if(ierr.ne.0) stop __LINE__

    write(*,*)"done cuda_mod_init"
!$OMP END MASTER
!$OMP BARRIER
  end subroutine cuda_mod_init




  !Meaningless kernel just to get initial kernel overheads out of the way.
  attributes(global) subroutine warmup(a)
    integer,value :: a
    a = 2.0 * a
  end subroutine warmup




  subroutine euler_step_cuda( np1_qdp , n0_qdp , dt , elem , hvcoord , hybrid , deriv , nets , nete , DSSopt , rhs_multiplier )
  use kinds             , only: real_kind
  use dimensions_mod    , only: np, npdg, nlev, qsize
  use hybrid_mod        , only: hybrid_t
  use element_mod       , only: element_t
  use derivative_mod    , only: derivative_t, divergence_sphere, gradient_sphere, vorticity_sphere
  use edge_mod          , only: edgevpack, edgevunpack
  use bndry_mod         , only: bndry_exchangev
  use hybvcoord_mod     , only: hvcoord_t
  use control_mod       , only:  nu_q, nu_p, limiter_option
  use perf_mod          , only: t_startf, t_stopf  ! _EXTERNAL
  use viscosity_mod     , only: biharmonic_wk_scalar, biharmonic_wk_scalar_minmax, neighbor_minmax
  implicit none
  integer              , intent(in   )         :: np1_qdp, n0_qdp
  real (kind=real_kind), intent(in   )         :: dt
  type (element_t)     , intent(inout), target :: elem(:)
  type (hvcoord_t)     , intent(in   )         :: hvcoord
  type (hybrid_t)      , intent(in   )         :: hybrid
  type (derivative_t)  , intent(in   )         :: deriv
  integer              , intent(in   )         :: nets
  integer              , intent(in   )         :: nete
  integer              , intent(in   )         :: DSSopt
  integer              , intent(in   )         :: rhs_multiplier

  ! local
  real(kind=real_kind), dimension(np,np                       ) :: divdp, dpdiss
  real(kind=real_kind), dimension(np,np,2                     ) :: gradQ
  real(kind=real_kind), dimension(np,np,2,nlev                ) :: Vstar
  real(kind=real_kind), dimension(np,np  ,nlev                ) :: Qtens
  real(kind=real_kind), dimension(np,np  ,nlev                ) :: dp,dp_star
  real(kind=real_kind), dimension(np,np  ,nlev,qsize,nets:nete) :: Qtens_biharmonic
  real(kind=real_kind), pointer, dimension(:,:,:)               :: DSSvar
  real(kind=real_kind) :: dp0
  integer :: ie,q,i,j,k
  integer :: rhs_viss = 0

  integer :: ierr
  type(dim3) :: blockdim , griddim

  call t_startf('euler_step')

  !This is a departure from the original order, adding an extra MPI communication. It's advantageous because it simplifies
  !the Pack-Exchange-Unpack procedure for us, since we're adding complexity to overlap MPI and packing
  if ( DSSopt /= DSSno_var ) then
    do ie = nets , nete
      if ( DSSopt == DSSeta         ) DSSvar => elem(ie)%derived%eta_dot_dpdn(:,:,:)
      if ( DSSopt == DSSomega       ) DSSvar => elem(ie)%derived%omega_p(:,:,:)
      if ( DSSopt == DSSdiv_vdp_ave ) DSSvar => elem(ie)%derived%divdp_proj(:,:,:)
      do k = 1 , nlev
        DSSvar(:,:,k) = elem(ie)%spheremp(:,:)*DSSvar(:,:,k) 
      enddo
      call edgeVpack(edgeAdvDSS,DSSvar(:,:,1:nlev),nlev,0,elem(ie)%desc)
    enddo
    call bndry_exchangeV(hybrid,edgeAdvDSS)
    do ie = nets , nete
      if ( DSSopt == DSSeta         ) DSSvar => elem(ie)%derived%eta_dot_dpdn(:,:,:)
      if ( DSSopt == DSSomega       ) DSSvar => elem(ie)%derived%omega_p(:,:,:)
      if ( DSSopt == DSSdiv_vdp_ave ) DSSvar => elem(ie)%derived%divdp_proj(:,:,:)
      call edgeVunpack(edgeAdvDSS,DSSvar(:,:,1:nlev),nlev,0,elem(ie)%desc)
      do k = 1 , nlev
        DSSvar(:,:,k)=DSSvar(:,:,k)*elem(ie)%rspheremp(:,:)
      enddo
    enddo
  endif

  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  !   compute Q min/max values for lim8
  !   compute biharmonic mixing term f
  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  rhs_viss = 0
  if ( limiter_option == 8 .or. nu_p > 0 ) then
    do ie = nets , nete
      ! add hyperviscosity to RHS.  apply to Q at timelevel n0, Qdp(n0)/dp
      do k = 1 , nlev    !  Loop index added with implicit inversion (AAM)
        dp(:,:,k) = elem(ie)%derived%dp(:,:,k) - rhs_multiplier*dt*elem(ie)%derived%divdp_proj(:,:,k) 
        do q = 1 , qsize
          Qtens_biharmonic(:,:,k,q,ie) = elem(ie)%state%Qdp(:,:,k,q,n0_qdp)/dp(:,:,k)
        enddo
      enddo
    enddo

    ! compute element qmin/qmax
    if ( rhs_multiplier == 0 ) then
      do ie = nets , nete
        do k = 1 , nlev    
          do q = 1 , qsize
            qmin(k,q,ie)=minval(Qtens_biharmonic(:,:,k,q,ie))
            qmax(k,q,ie)=maxval(Qtens_biharmonic(:,:,k,q,ie))
            qmin(k,q,ie)=max(qmin(k,q,ie),0d0)
          enddo
        enddo
      enddo
      ! update qmin/qmax based on neighbor data for lim8
      if ( limiter_option == 8 ) call neighbor_minmax(elem,hybrid,edgeAdvQ2,nets,nete,qmin(:,:,nets:nete),qmax(:,:,nets:nete))
    endif

    ! lets just reuse the old neighbor min/max, but update based on local data
    if ( rhs_multiplier == 1 ) then
      do ie = nets , nete
        do k = 1 , nlev    !  Loop index added with implicit inversion (AAM)
          do q = 1 , qsize
            qmin(k,q,ie)=min(qmin(k,q,ie),minval(Qtens_biharmonic(:,:,k,q,ie)))
            qmin(k,q,ie)=max(qmin(k,q,ie),0d0)
            qmax(k,q,ie)=max(qmax(k,q,ie),maxval(Qtens_biharmonic(:,:,k,q,ie)))
          enddo
        enddo
      enddo
    endif

    ! get niew min/max values, and also compute biharmonic mixing term
    if ( rhs_multiplier == 2 ) then
      rhs_viss = 3
      ! compute element qmin/qmax  
      do ie = nets , nete
        do k = 1  ,nlev    
          do q = 1 , qsize
            qmin(k,q,ie)=minval(Qtens_biharmonic(:,:,k,q,ie))
            qmax(k,q,ie)=maxval(Qtens_biharmonic(:,:,k,q,ie))
            qmin(k,q,ie)=max(qmin(k,q,ie),0d0)
          enddo
        enddo
      enddo
      ! two scalings depending on nu_p:
      ! nu_p=0:    qtens_biharmonic *= dp0                   (apply viscsoity only to q)
      ! nu_p>0):   qtens_biharmonc *= elem()%psdiss_ave      (for consistency, if nu_p=nu_q)
      if ( nu_p > 0 ) then
        do ie = nets , nete
          do k = 1 , nlev    
            dp0 = ( hvcoord%hyai(k+1) - hvcoord%hyai(k) )*hvcoord%ps0 + ( hvcoord%hybi(k+1) - hvcoord%hybi(k) )*hvcoord%ps0
            dpdiss(:,:) = ( hvcoord%hyai(k+1) - hvcoord%hyai(k) )*hvcoord%ps0 + ( hvcoord%hybi(k+1) - hvcoord%hybi(k) )*elem(ie)%derived%psdiss_ave(:,:)
            do q = 1 , qsize
              ! NOTE: divide by dp0 since we multiply by dp0 below
              Qtens_biharmonic(:,:,k,q,ie)=Qtens_biharmonic(:,:,k,q,ie)*dpdiss(:,:)/dp0
            enddo
          enddo
        enddo
      endif
      if ( limiter_option == 8 ) then
        ! biharmonic and update neighbor min/max
        call biharmonic_wk_scalar_minmax( elem , qtens_biharmonic , deriv , edgeAdvQ3 , hybrid , nets , nete , qmin(:,:,nets:nete) , qmax(:,:,nets:nete) )
      else
        ! regular biharmonic, no need to updat emin/max
        call biharmonic_wk_scalar( elem , qtens_biharmonic , deriv , edgeAdv , hybrid , nets , nete )
      endif
      do ie = nets , nete
        do k = 1 , nlev    !  Loop inversion (AAM)
          dp0 = ( hvcoord%hyai(k+1) - hvcoord%hyai(k) )*hvcoord%ps0 + ( hvcoord%hybi(k+1) - hvcoord%hybi(k) )*hvcoord%ps0
          do q = 1 , qsize
            ! note: biharmonic_wk() output has mass matrix already applied. Un-apply since we apply again below:
            qtens_biharmonic(:,:,k,q,ie) = -rhs_viss*dt*nu_q*dp0*Qtens_biharmonic(:,:,k,q,ie) / elem(ie)%spheremp(:,:)
          enddo
        enddo
      enddo
    endif
  endif  ! compute biharmonic mixing term and qmin/qmax

  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  !   2D Advection step
  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
  do ie = nets , nete
    ! Compute velocity used to advance Qdp 
    do k = 1 , nlev    !  Loop index added (AAM)
      ! derived variable divdp_proj() (DSS'd version of divdp) will only be correct on 2nd and 3rd stage
      ! but that's ok because rhs_multiplier=0 on the first stage:
      dp(:,:,k) = elem(ie)%derived%dp(:,:,k) - rhs_multiplier * dt * elem(ie)%derived%divdp_proj(:,:,k) 
      Vstar(:,:,1,k) = elem(ie)%derived%vn0(:,:,1,k) / dp(:,:,k)
      Vstar(:,:,2,k) = elem(ie)%derived%vn0(:,:,2,k) / dp(:,:,k)
    enddo

    ! advance Qdp
    do q = 1 , qsize
      do k = 1 , nlev  !  dp_star used as temporary instead of divdp (AAM)
        ! div( U dp Q), 
        gradQ(:,:,1) = Vstar(:,:,1,k) * elem(ie)%state%Qdp(:,:,k,q,n0_qdp)
        gradQ(:,:,2) = Vstar(:,:,2,k) * elem(ie)%state%Qdp(:,:,k,q,n0_qdp)
        dp_star(:,:,k) = divergence_sphere( gradQ , deriv , elem(ie) )
        Qtens(:,:,k) = elem(ie)%state%Qdp(:,:,k,q,n0_qdp) - dt * dp_star(:,:,k)
        ! optionally add in hyperviscosity computed above:
        if ( rhs_viss /= 0 ) Qtens(:,:,k) = Qtens(:,:,k) + Qtens_biharmonic(:,:,k,q,ie)
      enddo
         
      if ( limiter_option == 8 ) then
        do k = 1 , nlev  ! Loop index added (AAM)
          ! UN-DSS'ed dp at timelevel n0+1:  
          dp_star(:,:,k) = dp(:,:,k) - dt * elem(ie)%derived%divdp(:,:,k)  
          if ( nu_p > 0 .and. rhs_viss /= 0 ) then
            ! add contribution from UN-DSS'ed PS dissipation
            dpdiss(:,:) = ( hvcoord%hybi(k+1) - hvcoord%hybi(k) ) * elem(ie)%derived%psdiss_biharmonic(:,:)
            dp_star(:,:,k) = dp_star(:,:,k) - rhs_viss * dt * nu_q * dpdiss(:,:) / elem(ie)%spheremp(:,:)
          endif
        enddo
        ! apply limiter to Q = Qtens / dp_star 
        call limiter_optim_iter_full( Qtens(:,:,:) , elem(ie)%spheremp(:,:) , qmin(:,q,ie) , qmax(:,q,ie) , dp_star(:,:,:) )
      endif

      ! apply mass matrix, overwrite np1 with solution:
      ! dont do this earlier, since we allow np1 to be the same as n0
      ! and we dont want to overwrite n0 until we are done using it
      do k = 1 , nlev
        elem(ie)%state%Qdp(:,:,k,q,np1_qdp) = elem(ie)%spheremp(:,:) * Qtens(:,:,k) 
      enddo
    enddo
  enddo



  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!$OMP BARRIER
!$OMP MASTER
  ierr = cudaThreadSynchronize()
  do ie = 1,nelemd
    ierr = cudaMemcpy( qdp_d(1,1,1,1,np1_qdp,ie) , elem(ie)%state%qdp(1,1,1,1,np1_qdp) , size(elem(ie)%state%qdp(:,:,:,:,np1_qdp)) , cudaMemcpyHostToDevice )
  enddo
  ierr = cudaThreadSynchronize()

  if ( limiter_option == 4 ) then
    blockdim = dim3( np, np, nlev )
    griddim  = dim3( qsize_d, nelemd , 1 )
    call limiter2d_zero_kernel<<<griddim,blockdim>>>( qdp_d , 1 , nelemd , np1_qdp )
  endif
  call pack_exchange_unpack_stage(np1_qdp,hybrid,qdp_d,timelevels)
  blockdim = dim3( np*np   , nlev   , 1 )
  griddim  = dim3( qsize_d , nelemd , 1 )
  call euler_hypervis_kernel_last<<<griddim,blockdim>>>( qdp_d , rspheremp_d , 1 , nelemd , np1_qdp )


  ierr = cudaThreadSynchronize()
  do ie = 1,nelemd
    ierr = cudaMemcpy( elem(ie)%state%qdp(1,1,1,1,np1_qdp) , qdp_d(1,1,1,1,np1_qdp,ie) , size(elem(ie)%state%qdp(:,:,:,:,np1_qdp)) , cudaMemcpyDeviceToHost )
  enddo
  ierr = cudaThreadSynchronize()
!$OMP END MASTER
!$OMP BARRIER
  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

  call t_stopf('euler_step')
end subroutine euler_step_cuda




attributes(global) subroutine limiter2d_zero_kernel(Qdp,nets,nete,np1)
  use kinds, only : real_kind
  use dimensions_mod, only : np, nlev
  implicit none
  real (kind=real_kind), intent(inout) :: Qdp(np,np,nlev,qsize_d,timelevels,nets:nete)
  integer, value       , intent(in   ) :: nets,nete,np1
  integer :: i, j, k, q, ie, jj, tid, ind
  real (kind=real_kind) :: mass,mass_new
  real (kind=real_kind), shared :: Qdp_shared((np*np+PAD)*nlev)
  real (kind=real_kind), shared :: mass_shared(nlev)
  real (kind=real_kind), shared :: mass_new_shared(nlev)

  i  = threadidx%x
  j  = threadidx%y
  k  = threadidx%z
  q  = blockidx%x
  ie = blockidx%y

  tid = (threadidx%z-1)*(np*np    ) + (threadidx%y-1)*(np) + threadidx%x
  ind = (threadidx%z-1)*(np*np+PAD) + (threadidx%y-1)*(np) + threadidx%x

  Qdp_shared(ind) = Qdp(i,j,k,q,np1,ie)
  call syncthreads()

  if ( tid <= nlev ) then
    mass = 0.
    do jj = 1 , np*np
      mass = mass + Qdp_shared((tid-1)*(np*np+PAD)+jj)
    enddo
    mass_shared(tid) = mass
  endif
  call syncthreads()

  if ( mass_shared(k)  < 0 ) Qdp_shared(ind) = -Qdp_shared(ind)
  if ( Qdp_shared(ind) < 0 ) Qdp_shared(ind) = 0
  call syncthreads()

  if ( tid <= nlev ) then
    mass = 0.
    do jj = 1 , np*np
      mass = mass + Qdp_shared((tid-1)*(np*np+PAD)+jj)
    enddo
    mass_new_shared(tid) = mass
  endif
  call syncthreads()

  ! now scale the all positive values to restore mass
  if ( mass_new_shared(k) > 0 ) Qdp_shared(ind) =  Qdp_shared(ind) * abs(mass_shared(k)) / mass_new_shared(k)
  if ( mass_shared    (k) < 0 ) Qdp_shared(ind) = -Qdp_shared(ind)
  Qdp(i,j,k,q,np1,ie) = Qdp_shared(ind)
end subroutine limiter2d_zero_kernel




attributes(global) subroutine euler_hypervis_kernel_last( Qdp , rspheremp , nets , nete , np1 )
  implicit none
  real(kind=real_kind), dimension(np*np,nlev,qsize_d,timelevels,nets:nete), intent(inout) :: Qdp
  real(kind=real_kind), dimension(np*np                        ,nets:nete), intent(in   ) :: rspheremp
  integer, value                                                          , intent(in   ) :: nets , nete , np1
  integer :: i, k, q, ie
  i  = threadidx%x
  k  = threadidx%y
  q  = blockidx%x
  ie = blockidx%y
  Qdp(i,k,q,np1,ie) = rspheremp(i,ie) * Qdp(i,k,q,np1,ie)
end subroutine euler_hypervis_kernel_last




subroutine pack_exchange_unpack_stage(np1,hybrid,array_in,tl_in)
  use hybrid_mod, only : hybrid_t
  use schedule_mod, only : schedule_t, schedule, cycle_t
  use parallel_mod, only : abortmp, status, srequest, rrequest, mpireal_t, mpiinteger_t, iam
  implicit none
  include 'mpif.h'
  type(hybrid_t)              , intent(in   ) :: hybrid
  real(kind=real_kind), device, intent(inout) :: array_in(np,np,nlev,qsize_d,tl_in,nelemd)
  integer, value              , intent(in   ) :: np1 , tl_in
  ! local
  type(dim3)                :: griddim6,blockdim6
  integer                   :: icycle,nSendCycles,nRecvCycles,n, ierr
  type (Schedule_t),pointer :: pSchedule
  type (Cycle_t),pointer    :: pCycle
  integer                   :: dest,length,tag,iptr,source,nlyr,query_sum, npacked
  logical :: recvflag, internal_unpacked
  real :: time_milli
#ifdef _PREDICT
   pSchedule => Schedule(iam)
#else
   pSchedule => Schedule(1)
#endif
   nlyr = edgeAdv%nlyr
   nSendCycles = pSchedule%nSendCycles
   nRecvCycles = pSchedule%nRecvCycles
   d2h_done = .false.
   msg_sent = .false.
   h2d_done = .false.
   internal_unpacked = .false.
   nmsg_rcvd = 0
   nmsg_sent = 0


  ierr = cudaThreadSynchronize()
  do icycle=1,nRecvCycles
    pCycle => pSchedule%RecvCycle(icycle)
    source  = pCycle%source - 1
    length  = nlyr * pCycle%lengthP
    tag     = pCycle%tag
    call MPI_Irecv(recvbuf_h(1,1,icycle),length,MPIreal_t,source,tag,hybrid%par%comm,Rrequest(icycle),ierr)
  enddo
  if (old_peu) then
    blockdim6 = dim3( np      , np     , nlev )
    griddim6  = dim3( qsize_d , nelemd , 1    )
    call edgeVpack_kernel<<<griddim6,blockdim6>>>(edgebuf_d,array_in,putmapP_d,reverse_d,nbuf,0,1,nelemd,np1,tl_in)
    ierr = cudaThreadSynchronize()
  else
    do icycle = 1 , nSendCycles
      blockdim6 = dim3( np      , np                 , nlev )
      griddim6  = dim3( qsize_d , send_nelem(icycle) , 1    )
      call edgeVpack_kernel_stage<<<griddim6,blockdim6,0,streams2(icycle)>>>(edgebuf_d,array_in,putmapP_d,reverse_d,nbuf,0,1,nelemd,np1,send_indices_d,nSendCycles,icycle,tl_in)
    enddo
  endif
  do while ( nmsg_rcvd < nRecvCycles .or. nmsg_sent < nSendCycles .or. .not. internal_unpacked )
    !When this cycle's D2H memcpy is finished, call the MPI_Isend to shoot it over to the destination process
    do icycle = 1 , nSendCycles
      if ( .not. d2h_done(icycle) ) then
        if ( cudaStreamQuery(streams2(icycle)) == 0 ) then
          pCycle => pSchedule%SendCycle(icycle)
          iptr   =  pCycle%ptrP
          ierr = cudaMemcpyAsync(sendbuf_h(1,1,icycle),edgebuf_d(1,iptr),size(sendbuf_h(1:nlyr,1:pCycle%lengthP,icycle)),cudaMemcpyDeviceToHost,streams(icycle))
          d2h_done(icycle) = .true.
        endif
      endif
      if ( .not. msg_sent(icycle) ) then  !Only send once per cycle
        if ( d2h_done(icycle) ) then
          if ( cudaStreamQuery(streams(icycle)) == 0 ) then
            pCycle => pSchedule%SendCycle(icycle)
            dest   =  pCycle%dest - 1
            iptr   =  pCycle%ptrP
            length =  nlyr * pCycle%lengthP
            tag    =  pCycle%tag
            call MPI_Isend(sendbuf_h(1,1,icycle),length,MPIreal_t,dest,tag,hybrid%par%comm,Srequest(icycle),ierr)
            msg_sent(icycle) = .true.
            nmsg_sent = nmsg_sent + 1
          endif
        endif
      endif
    enddo
    if (.not. internal_unpacked) then
      if (nmsg_sent == nSendCycles) then
        blockdim6 = dim3( np      , np                  , nlev )
        griddim6  = dim3( qsize_d , recv_internal_nelem , 1    )
        call edgeVunpack_kernel_stage<<<griddim6,blockdim6>>>(edgebuf_d,array_in,getmapP_d,nbuf,0,1,nelemd,np1,recv_internal_indices_d,tl_in)
        internal_unpacked = .true.
      endif
    endif
    !When this cycle's MPI transfer is compliete, then call the D2H memcopy asynchronously
    do icycle = 1 , nRecvCycles
      if ( .not. h2d_done(icycle) ) then  !Only host to device once per cycle
        call MPI_Test(Rrequest(icycle),recvflag,status,ierr)
        if ( (ierr==MPI_SUCCESS) .and. recvflag ) then
          pCycle => pSchedule%RecvCycle(icycle)
          iptr   =  pCycle%ptrP
          ierr = cudaMemcpyAsync(recvbuf_d(1,1,icycle),recvbuf_h(1,1,icycle),size(recvbuf_h(1:nlyr,1:pCycle%lengthP,icycle)),cudaMemcpyHostToDevice,streams(icycle))
          h2d_done(icycle) = .true.
          nmsg_rcvd = nmsg_rcvd + 1 !This is how we close the polling loop, once every message has been received
        endif
      endif
    enddo
  enddo
  call MPI_WaitAll(nSendCycles,Srequest,status,ierr)
  do icycle = 1 , nRecvCycles
    pCycle => pSchedule%RecvCycle(icycle)
    iptr   =  pCycle%ptrP
    ierr = cudaMemcpyAsync(edgebuf_d(1,iptr),recvbuf_d(1,1,icycle),size(recvbuf_h(1:nlyr,1:pCycle%lengthP,icycle)),cudaMemcpyDeviceToDevice,streams(icycle))
  enddo
  ierr = cudaThreadSynchronize()
  blockdim6 = dim3( np      , np                  , nlev )
  griddim6  = dim3( qsize_d , recv_external_nelem , 1    )
  call edgeVunpack_kernel_stage<<<griddim6,blockdim6>>>(edgebuf_d,array_in,getmapP_d,nbuf,0,1,nelemd,np1,recv_external_indices_d,tl_in)


! ierr = cudaThreadSynchronize()
! do icycle=1,nRecvCycles
!   pCycle => pSchedule%RecvCycle(icycle)
!   source  = pCycle%source - 1
!   length  = nlyr * pCycle%lengthP
!   tag     = pCycle%tag
!   call MPI_Irecv(recvbuf_h(1,1,icycle),length,MPIreal_t,source,tag,hybrid%par%comm,Rrequest(icycle),ierr)
! enddo
! blockdim6 = dim3( np      , np     , nlev )
! griddim6  = dim3( qsize_d , nelemd , 1    )
! call edgeVpack_kernel<<<griddim6,blockdim6>>>(edgebuf_d,array_in,putmapP_d,reverse_d,nbuf,0,1,nelemd,np1,tl_in)
! ierr = cudaThreadSynchronize()
! do icycle = 1 , nSendCycles
!   pCycle => pSchedule%SendCycle(icycle)
!   iptr   =  pCycle%ptrP
!   ierr = cudaMemcpyAsync(sendbuf_h(1,1,icycle),edgebuf_d(1,iptr),size(sendbuf_h(1:nlyr,1:pCycle%lengthP,icycle)),cudaMemcpyDeviceToHost,streams(icycle))
! enddo
! ierr = cudaThreadSynchronize()
! do icycle = 1 , nSendCycles
!   pCycle => pSchedule%SendCycle(icycle)
!   dest   =  pCycle%dest - 1
!   iptr   =  pCycle%ptrP
!   length =  nlyr * pCycle%lengthP
!   tag    =  pCycle%tag
!   call MPI_Isend(sendbuf_h(1,1,icycle),length,MPIreal_t,dest,tag,hybrid%par%comm,Srequest(icycle),ierr)
! enddo
! call MPI_WaitAll(nRecvCycles,Rrequest,status,ierr)
! call MPI_WaitAll(nSendCycles,Srequest,status,ierr)
! !When this cycle's MPI transfer is compliete, then call the D2H memcopy asynchronously
! do icycle = 1 , nRecvCycles
!   pCycle => pSchedule%RecvCycle(icycle)
!   iptr   =  pCycle%ptrP
!   ierr = cudaMemcpyAsync(edgebuf_d(1,iptr),recvbuf_h(1,1,icycle),size(recvbuf_h(1:nlyr,1:pCycle%lengthP,icycle)),cudaMemcpyHostToDevice,streams(icycle))
! enddo
! blockdim6 = dim3( np      , np     , nlev )
! griddim6  = dim3( qsize_d , nelemd , 1    )
! ierr = cudaThreadSynchronize()
! call edgeVunpack_kernel<<<griddim6,blockdim6>>>(edgebuf_d,array_in,getmapP_d,nbuf,0,1,nelemd,np1,tl_in)
! ierr = cudaThreadSynchronize()

end subroutine pack_exchange_unpack_stage




attributes(global) subroutine edgeVpack_kernel_stage(edgebuf,v,putmapP,reverse,nbuf,kptr,nets,nete,nt,send_indices,nSendCycles,icycle,tl_in)
  use control_mod, only : north, south, east, west, neast, nwest, seast, swest
  implicit none
  real (kind=real_kind), intent(  out) :: edgebuf(nlev*qsize_d,nbuf)
  integer              , intent(in   ) :: putmapP(max_neigh_edges,nets:nete)
  logical              , intent(in   ) :: reverse(max_neigh_edges,nets:nete)
  real (kind=real_kind), intent(in   ) :: v(np*np,nlev,qsize_d,tl_in,nets:nete)
  integer              , intent(in   ) :: send_indices(nets:nete,nSendCycles)
  integer, value       , intent(in   ) :: kptr,nets,nete,nt,nbuf,nSendCycles,icycle,tl_in
  integer :: i,j,k,q,l,offset,ij,ijk,ti,tj,tk,x,y,ir,  reverse_south, reverse_north, reverse_west, reverse_east, el
  integer, shared :: ic(max_corner_elem,4), direction(4), reverse_direction(4)
  real (kind=real_kind), shared :: vshrd(nlev+PAD,np,np)
  i  = threadidx%x
  j  = threadidx%y
  k  = threadidx%z
  q  = blockidx%x
  el = send_indices(blockidx%y,icycle)
  
  ij = (j-1)*np+i
  ijk = (k-1)*np*np + (j-1)*np + i -1
  tk = mod( ijk, nlev ) + 1
  ti = mod( ijk/nlev, np ) + 1
  tj = ijk/(nlev*np) + 1

  if( i+j+k == blockdim%x + blockdim%y + blockdim%z ) then
    direction(west_px)  = putmapP(west ,el)
    direction(east_px)  = putmapP(east ,el)
    direction(south_px) = putmapP(south,el)
    direction(north_px) = putmapP(north,el)
    reverse_direction(south_px) = reverse(south,el)
    reverse_direction(north_px) = reverse(north,el)
    reverse_direction(west_px)  = reverse(west,el)
    reverse_direction(east_px)  = reverse(east,el)
  endif
  if( ijk < max_corner_elem ) then
    ic(ijk+1,1) = putmapP(swest+ijk,el)+1
    ic(ijk+1,2) = putmapP(seast+ijk,el)+1
    ic(ijk+1,3) = putmapP(nwest+ijk,el)+1
    ic(ijk+1,4) = putmapP(neast+ijk,el)+1
  endif
  vshrd(k,i,j) = v(ij,k,q,nt,el)
  
  call syncthreads()
   
  offset = (q-1)*nlev + tk + kptr
  ir = np-ti+1
  if( 1==tj .or. 4==tj ) then
    if( reverse_direction(tj) ) then; edgebuf(offset,direction(tj)+ti) = vshrd(tk,ir,tj)
    else                            ; edgebuf(offset,direction(tj)+ti) = vshrd(tk,ti,tj); endif
  endif
  if( 2==tj ) then
    if( reverse_direction(2) ) then; edgebuf(offset,direction(tj)+ti) = vshrd(tk,1,ir)
    else                           ; edgebuf(offset,direction(tj)+ti) = vshrd(tk,1,ti); endif
  endif
  if( 3==tj ) then
    if( reverse_direction(3) ) then; edgebuf(offset,direction(tj)+ti) = vshrd(tk,4,ir)
    else                           ; edgebuf(offset,direction(tj)+ti) = vshrd(tk,4,ti); endif
  endif
  if( tj==1 ) then
    do l=1, max_corner_elem       
      x = mod(ti-1,2)*(np-1) + 1  ! we need to convert ti index from {1,2,3,4} to {(1,1),(4,1),(1,4),(4,4)}
      y = ((ti-1)/2)*(np-1) + 1   !   so, ti->(x,y)
      if( ic(l,ti) /= 0 ) edgebuf( offset, ic(l,ti) ) = vshrd(tk,x,y)
    enddo
  endif
end subroutine edgeVpack_kernel_stage



attributes(global) subroutine edgeVunpack_kernel_stage(edgebuf,v,getmapP,nbuf,kptr,nets,nete,nt,recv_indices,tl_in)
  use control_mod, only : north, south, east, west, neast, nwest, seast, swest
  implicit none
  real (kind=real_kind), intent(in   ) :: edgebuf(nlev*qsize_d,nbuf)
  integer              , intent(in   ) :: getmapP(max_neigh_edges,nets:nete)
  real (kind=real_kind), intent(inout) :: v(np*np,nlev,qsize_d,tl_in,nets:nete)
  integer              , intent(in   ) :: recv_indices(nets:nete)
  integer, value       , intent(in   ) :: kptr,nets,nete,nt,nbuf,tl_in
  integer :: i,j,k,l,q,el,offset,ij,ti,tj,tk,ijk,x,y,tij
  integer, shared :: direction(4),is,ie,in,iw, ic(max_corner_elem,4)
  real (kind=real_kind), shared :: vshrd(nlev+PAD,np,np)
  real (kind=real_kind) :: v_before_update, neighbor_value
  
  i  = threadidx%x
  j  = threadidx%y
  k  = threadidx%z
  q  = blockidx%x
  el = recv_indices(blockidx%y)
  
  ij = (j-1)*np+i
  ijk = (k-1)*np*np + ij -1
  tk = mod( ijk, nlev ) + 1
  ti = mod( ijk/nlev, np ) + 1
  tj = ijk/(nlev*np) + 1
  
  if( i + j + k == np+np+nlev ) then
    direction(west_px)  = getmapP(west ,el)
    direction(east_px)  = getmapP(east ,el)
    direction(south_px) = getmapP(south,el)
    direction(north_px) = getmapP(north,el)
  endif
  if( ijk < max_corner_elem) then
    ic(ijk+1,1) = getmapP(swest+ijk,el)+1
    ic(ijk+1,2) = getmapP(seast+ijk,el)+1
    ic(ijk+1,3) = getmapP(nwest+ijk,el)+1
    ic(ijk+1,4) = getmapP(neast+ijk,el)+1
  endif
  vshrd(k,i,j) = 0.D0
  call syncthreads()
    
  offset = (q-1)*nlev + tk + kptr
  neighbor_value = edgebuf( offset, direction(tj)+ti ) ! load neighbor values into registers 
                                                       !  nlev x np consecutive threads contain all the face values
                                                       !   tj = 1:  south   
                                                       !   tj = 2:  west
                                                       !   tj = 3:  east
                                                       !   tj = 4:  north
                                                       
  ! combine the neighbor values in smem
  if( 1==tj .or. 4==tj ) vshrd(tk, ti, tj) = neighbor_value  ! add the south and north values to smem
  call syncthreads() ! this sync is needed to avoid race conditions (east/west share corners with sourth/north)
  if( 2==tj ) vshrd(tk,1,ti) = vshrd(tk,1,ti) + neighbor_value  ! update west
  if( 3==tj ) vshrd(tk,4,ti) = vshrd(tk,4,ti) + neighbor_value  ! update east
  call syncthreads()

  v_before_update = v(ij,k,q,nt,el) ! start loading the local value to be updated with neibhbor values
  
  ! update the "corner" columns
  if( tj==1 ) then
    do l=1, max_corner_elem       
      x = mod(ti-1,2)*(np-1) + 1  ! we need to convert ti index from {1,2,3,4} to {(1,1),(4,1),(1,4),(4,4)}
      y = ((ti-1)/2)*(np-1) + 1   !   so, ti->(x,y)
      if( ic(l,ti) /= 0 ) vshrd(tk,x,y) = vshrd(tk,x,y) + edgebuf( offset, ic(l,ti) )
    enddo
  endif
  call syncthreads()
    
  v(ij,k,q,nt,el) = v_before_update + vshrd(k,i,j)
end subroutine edgeVunpack_kernel_stage




attributes(global) subroutine edgeVpack_kernel(edgebuf,v,putmapP,reverse,nbuf,kptr,nets,nete,nt,tl_in)
  use control_mod, only : north, south, east, west, neast, nwest, seast, swest
  implicit none
  real (kind=real_kind), intent(  out) :: edgebuf(nlev*qsize_d,nbuf)
  integer              , intent(in   ) :: putmapP(max_neigh_edges,nets:nete)
  logical              , intent(in   ) :: reverse(max_neigh_edges,nets:nete)
  real (kind=real_kind), intent(in   ) :: v(np*np,nlev,qsize_d,tl_in,nets:nete)
  integer, value       , intent(in   ) :: kptr,nets,nete,nt,nbuf,tl_in
  integer :: i,j,k,q,l,offset,ij,ijk,ti,tj,tk,x,y,ir,  reverse_south, reverse_north, reverse_west, reverse_east, el
  integer, shared :: ic(max_corner_elem,4), direction(4), reverse_direction(4)
  real (kind=real_kind), shared :: vshrd(nlev+PAD,np,np)
  i  = threadidx%x
  j  = threadidx%y
  k  = threadidx%z
  q  = blockidx%x
  el = blockidx%y
  
  ij = (j-1)*np+i
  ijk = (k-1)*np*np + (j-1)*np + i -1
  tk = mod( ijk, nlev ) + 1
  ti = mod( ijk/nlev, np ) + 1
  tj = ijk/(nlev*np) + 1

  if( i+j+k == blockdim%x + blockdim%y + blockdim%z ) then
    direction(west_px)  = putmapP(west ,el)
    direction(east_px)  = putmapP(east ,el)
    direction(south_px) = putmapP(south,el)
    direction(north_px) = putmapP(north,el)
    reverse_direction(south_px) = reverse(south,el)
    reverse_direction(north_px) = reverse(north,el)
    reverse_direction(west_px)  = reverse(west,el)
    reverse_direction(east_px)  = reverse(east,el)
  endif
  if( ijk < max_corner_elem ) then
    ic(ijk+1,1) = putmapP(swest+ijk,el)+1
    ic(ijk+1,2) = putmapP(seast+ijk,el)+1
    ic(ijk+1,3) = putmapP(nwest+ijk,el)+1
    ic(ijk+1,4) = putmapP(neast+ijk,el)+1
  endif
  vshrd(k,i,j) = v(ij,k,q,nt,el)
  
  call syncthreads()
   
  offset = (q-1)*nlev + tk + kptr
  ir = np-ti+1
  if( 1==tj .or. 4==tj ) then
    if( reverse_direction(tj) ) then; edgebuf(offset,direction(tj)+ti) = vshrd(tk,ir,tj)
    else                            ; edgebuf(offset,direction(tj)+ti) = vshrd(tk,ti,tj); endif
  endif
  if( 2==tj ) then
    if( reverse_direction(2) ) then; edgebuf(offset,direction(tj)+ti) = vshrd(tk,1,ir)
    else                           ; edgebuf(offset,direction(tj)+ti) = vshrd(tk,1,ti); endif
  endif
  if( 3==tj ) then
    if( reverse_direction(3) ) then; edgebuf(offset,direction(tj)+ti) = vshrd(tk,4,ir)
    else                           ; edgebuf(offset,direction(tj)+ti) = vshrd(tk,4,ti); endif
  endif
  if( tj==1 ) then
    do l=1, max_corner_elem       
      x = mod(ti-1,2)*(np-1) + 1  ! we need to convert ti index from {1,2,3,4} to {(1,1),(4,1),(1,4),(4,4)}
      y = ((ti-1)/2)*(np-1) + 1   !   so, ti->(x,y)
      if( ic(l,ti) /= 0 ) edgebuf( offset, ic(l,ti) ) = vshrd(tk,x,y)
    enddo
  endif
end subroutine edgeVpack_kernel





attributes(global) subroutine edgeVunpack_kernel(edgebuf,v,getmapP,nbuf,kptr,nets,nete,nt,tl_in)
  use control_mod, only : north, south, east, west, neast, nwest, seast, swest
  real (kind=real_kind), intent(in   ) :: edgebuf(nlev*qsize_d,nbuf)
  integer              , intent(in   ) :: getmapP(max_neigh_edges,nets:nete)
  real (kind=real_kind), intent(inout) :: v(np*np,nlev,qsize_d,tl_in,nets:nete)
  integer, value       , intent(in   ) :: kptr,nets,nete,nt,nbuf,tl_in
  integer :: i,j,k,l,q,el,offset,ij,ti,tj,tk,ijk,x,y,tij
  integer, shared :: direction(4),is,ie,in,iw, ic(max_corner_elem,4)
  real (kind=real_kind), shared :: vshrd(nlev+PAD,np,np)
  real (kind=real_kind) :: v_before_update, neighbor_value
  
  i  = threadidx%x
  j  = threadidx%y
  k  = threadidx%z
  q  = blockidx%x
  el = blockidx%y
  
  ij = (j-1)*np+i
  ijk = (k-1)*np*np + ij -1
  tk = mod( ijk, nlev ) + 1
  ti = mod( ijk/nlev, np ) + 1
  tj = ijk/(nlev*np) + 1
  
  if( i + j + k == np+np+nlev ) then
    direction(west_px)  = getmapP(west ,el)
    direction(east_px)  = getmapP(east ,el)
    direction(south_px) = getmapP(south,el)
    direction(north_px) = getmapP(north,el)
  endif
  if( ijk < max_corner_elem) then
    ic(ijk+1,1) = getmapP(swest+ijk,el)+1
    ic(ijk+1,2) = getmapP(seast+ijk,el)+1
    ic(ijk+1,3) = getmapP(nwest+ijk,el)+1
    ic(ijk+1,4) = getmapP(neast+ijk,el)+1
  endif
  vshrd(k,i,j) = 0.D0
  call syncthreads()
    
  offset = (q-1)*nlev + tk + kptr
  neighbor_value = edgebuf( offset, direction(tj)+ti ) ! load neighbor values into registers 
                                                       !  nlev x np consecutive threads contain all the face values
                                                       !   tj = 1:  south   
                                                       !   tj = 2:  west
                                                       !   tj = 3:  east
                                                       !   tj = 4:  north
                                                       
  ! combine the neighbor values in smem
  if( 1==tj .or. 4==tj ) vshrd(tk, ti, tj) = neighbor_value  ! add the south and north values to smem
  call syncthreads() ! this sync is needed to avoid race conditions (east/west share corners with sourth/north)
  if( 2==tj ) vshrd(tk,1,ti) = vshrd(tk,1,ti) + neighbor_value  ! update west
  if( 3==tj ) vshrd(tk,4,ti) = vshrd(tk,4,ti) + neighbor_value  ! update east
  call syncthreads()

  v_before_update = v(ij,k,q,nt,el) ! start loading the local value to be updated with neibhbor values
  
  ! update the "corner" columns
  if( tj==1 ) then
    do l=1, max_corner_elem       
      x = mod(ti-1,2)*(np-1) + 1  ! we need to convert ti index from {1,2,3,4} to {(1,1),(4,1),(1,4),(4,4)}
      y = ((ti-1)/2)*(np-1) + 1   !   so, ti->(x,y)
      if( ic(l,ti) /= 0 ) vshrd(tk,x,y) = vshrd(tk,x,y) + edgebuf( offset, ic(l,ti) )
    enddo
  endif
  call syncthreads()
    
  v(ij,k,q,nt,el) = v_before_update + vshrd(k,i,j)
end subroutine edgeVunpack_kernel














subroutine limiter2d_zero(Q,hvcoord)
  ! mass conserving zero limiter (2D only).  to be called just before DSS
  !
  ! this routine is called inside a DSS loop, and so Q had already
  ! been multiplied by the mass matrix.  Thus dont include the mass
  ! matrix when computing the mass = integral of Q over the element
  !
  ! ps is only used when advecting Q instead of Qdp
  ! so ps should be at one timelevel behind Q
  use hybvcoord_mod     , only: hvcoord_t
  implicit none
  real (kind=real_kind), intent(inout) :: Q(np,np,nlev)
  type (hvcoord_t)     , intent(in   ) :: hvcoord

  ! local
  real (kind=real_kind) :: dp(np,np)
  real (kind=real_kind) :: mass,mass_new,ml
  integer i,j,k

  do k = nlev , 1 , -1
    mass = 0
    do j = 1 , np
      do i = 1 , np
        !ml = Q(i,j,k)*dp(i,j)*spheremp(i,j)  ! see above
        ml = Q(i,j,k)
        mass = mass + ml
      enddo
    enddo

    ! negative mass.  so reduce all postive values to zero 
    ! then increase negative values as much as possible
    if ( mass < 0 ) Q(:,:,k) = -Q(:,:,k) 
    mass_new = 0
    do j = 1 , np
      do i = 1 , np
        if ( Q(i,j,k) < 0 ) then
          Q(i,j,k) = 0
        else
          ml = Q(i,j,k)
          mass_new = mass_new + ml
        endif
      enddo
    enddo

    ! now scale the all positive values to restore mass
    if ( mass_new > 0 ) Q(:,:,k) = Q(:,:,k) * abs(mass) / mass_new
    if ( mass     < 0 ) Q(:,:,k) = -Q(:,:,k) 
  enddo
end subroutine limiter2d_zero




subroutine limiter_optim_iter_full(ptens,sphweights,minp,maxp,dpmass)
  !THIS IS A NEW VERSION OF LIM8, POTENTIALLY FASTER BECAUSE INCORPORATES KNOWLEDGE FROM
  !PREVIOUS ITERATIONS
  
  !The idea here is the following: We need to find a grid field which is closest
  !to the initial field (in terms of weighted sum), but satisfies the min/max constraints.
  !So, first we find values which do not satisfy constraints and bring these values
  !to a closest constraint. This way we introduce some mass change (addmass),
  !so, we redistribute addmass in the way that l2 error is smallest. 
  !This redistribution might violate constraints thus, we do a few iterations. 
  use kinds         , only : real_kind
  use dimensions_mod, only : np, np, nlev
  real (kind=real_kind), dimension(np,np,nlev), intent(inout)            :: ptens
  real (kind=real_kind), dimension(np,np     ), intent(in   )            :: sphweights
  real (kind=real_kind), dimension(      nlev), intent(inout)            :: minp
  real (kind=real_kind), dimension(      nlev), intent(inout)            :: maxp
  real (kind=real_kind), dimension(np,np,nlev), intent(in   ), optional  :: dpmass

  real (kind=real_kind), dimension(np,np,nlev) :: weights
  real (kind=real_kind), dimension(np,np     ) :: ptens_mass
  integer  k1, k, i, j, iter, i1, i2
  integer :: whois_neg(np*np), whois_pos(np*np), neg_counter, pos_counter
  real (kind=real_kind) :: addmass, weightssum, mass
  real (kind=real_kind) :: x(np*np),c(np*np)
  real (kind=real_kind) :: al_neg(np*np), al_pos(np*np), howmuch
  real (kind=real_kind) :: tol_limiter = 1e-15
  integer, parameter :: maxiter = 5

  do k = 1 , nlev
    weights(:,:,k) = sphweights(:,:) * dpmass(:,:,k)
    ptens(:,:,k) = ptens(:,:,k) / dpmass(:,:,k)
  enddo

  do k = 1 , nlev
    k1 = 1
    do i = 1 , np
      do j = 1 , np
        c(k1) = weights(i,j,k)
        x(k1) = ptens(i,j,k)
        k1 = k1 + 1
      enddo
    enddo

    mass = sum(c*x)

    ! relax constraints to ensure limiter has a solution:
    ! This is only needed if runnign with the SSP CFL>1 or 
    ! due to roundoff errors
    if( (mass / sum(c)) < minp(k) ) then
      minp(k) = mass / sum(c)
    endif
    if( (mass / sum(c)) > maxp(k) ) then
      maxp(k) = mass / sum(c)
    endif

    addmass = 0.0d0
    pos_counter = 0;
    neg_counter = 0;
    
    ! apply constraints, compute change in mass caused by constraints 
    do k1 = 1 , np*np
      if ( ( x(k1) >= maxp(k) ) ) then
        addmass = addmass + ( x(k1) - maxp(k) ) * c(k1)
        x(k1) = maxp(k)
        whois_pos(k1) = -1
      else
        pos_counter = pos_counter+1;
        whois_pos(pos_counter) = k1;
      endif
      if ( ( x(k1) <= minp(k) ) ) then
        addmass = addmass - ( minp(k) - x(k1) ) * c(k1)
        x(k1) = minp(k)
        whois_neg(k1) = -1
      else
        neg_counter = neg_counter+1;
        whois_neg(neg_counter) = k1;
      endif
    enddo
    
    ! iterate to find field that satifies constraints and is l2-norm closest to original 
    weightssum = 0.0d0
    if ( addmass > 0 ) then
      do i2 = 1 , maxIter
        weightssum = 0.0
        do k1 = 1 , pos_counter
          i1 = whois_pos(k1)
          weightssum = weightssum + c(i1)
          al_pos(i1) = maxp(k) - x(i1)
        enddo
        
        if( ( pos_counter > 0 ) .and. ( addmass > tol_limiter * abs(mass) ) ) then
          do k1 = 1 , pos_counter
            i1 = whois_pos(k1)
            howmuch = addmass / weightssum
            if ( howmuch > al_pos(i1) ) then
              howmuch = al_pos(i1)
              whois_pos(k1) = -1
            endif
            addmass = addmass - howmuch * c(i1)
            weightssum = weightssum - c(i1)
            x(i1) = x(i1) + howmuch
          enddo
          !now sort whois_pos and get a new number for pos_counter
          !here neg_counter and whois_neg serve as temp vars
          neg_counter = pos_counter
          whois_neg = whois_pos
          whois_pos = -1
          pos_counter = 0
          do k1 = 1 , neg_counter
            if ( whois_neg(k1) .ne. -1 ) then
              pos_counter = pos_counter+1
              whois_pos(pos_counter) = whois_neg(k1)
            endif
          enddo
        else
          exit
        endif
      enddo
    else
       do i2 = 1 , maxIter
         weightssum = 0.0
         do k1 = 1 , neg_counter
           i1 = whois_neg(k1)
           weightssum = weightssum + c(i1)
           al_neg(i1) = x(i1) - minp(k)
         enddo
         
         if ( ( neg_counter > 0 ) .and. ( (-addmass) > tol_limiter * abs(mass) ) ) then
           do k1 = 1 , neg_counter
             i1 = whois_neg(k1)
             howmuch = -addmass / weightssum
             if ( howmuch > al_neg(i1) ) then
               howmuch = al_neg(i1)
               whois_neg(k1) = -1
             endif
             addmass = addmass + howmuch * c(i1)
             weightssum = weightssum - c(i1)
             x(i1) = x(i1) - howmuch
           enddo
           !now sort whois_pos and get a new number for pos_counter
           !here pos_counter and whois_pos serve as temp vars
           pos_counter = neg_counter
           whois_pos = whois_neg
           whois_neg = -1
           neg_counter = 0
           do k1 = 1 , pos_counter
             if ( whois_pos(k1) .ne. -1 ) then
               neg_counter = neg_counter+1
               whois_neg(neg_counter) = whois_pos(k1)
             endif
           enddo
         else
           exit
         endif
       enddo
    endif
    
    k1 = 1
    do i = 1 , np
      do j = 1 , np
        ptens(i,j,k) = x(k1)
        k1 = k1+1
      enddo
    enddo
    
 enddo
 
 do k = 1 , nlev
   ptens(:,:,k) = ptens(:,:,k) * dpmass(:,:,k)
 enddo
end subroutine limiter_optim_iter_full





#endif
end module cuda_mod


